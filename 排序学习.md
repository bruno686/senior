排序学习（Learning to Rank，LTR），也称机器排序学习（Machine-learned Ranking，MLR) ，就是==使用机器学习的技术解决排序问题==。

------

##### 一 传统排序方法

1. **基于相似度**

   布尔模型）、VSM（如 TF-IDF、LSI）、概率排序思想（如 BM25、LMIR）

2. **基于重要性**

​	  利用 doc 本身的重要性，如 PageRank、TrustRank 等

**二 推荐的整个流程可以分为召回、排序、重排序这三个阶段**

- **召回**就是找到用户可能喜欢的几百条资讯，
- **排序**就是对这几百条资讯利用机器学习的方法预估用户对每条资讯的==偏好程度==，一般以点击率衡量，也就是点击率预估问题。不难看出，排序学习在推荐领域主要用于排序阶段，最常用的是 Pointwise 排序方法；
- **重排序**更多是考虑业务逻辑的==规则过滤==，如在推荐结果的多样性、时效性、新颖性等方面进行控制

**三 推荐系统中排序学习的必要性**

在没有 Learning to Rank 之前，基于内容的推荐算法和基于邻域的协同过滤虽然也能预测用户的偏好，可以帮助用户召回大量的物品，但是我们必须知道，**推荐系统中更重要的目标是排序，因为真正最后推荐给用户的只有少数物品，我们更关心这些召回物品中哪些才是用户心中更加喜欢的，也就是排序更靠前，这便是 Top-N 推荐**。

**四 排序学习框架**

> 无论是搜索还是推荐，排序学习模型的**特征提取**以及**标签获取**是非常重要的两个过程

1. **特征提取**

   在排序学习模型中，文档都是转化成特征向量来表征的

   **一是文档本身的特征**，比如 Pagerank 值、内容丰富度、spam 值、number of slash、url length、inlink number、outlink number、siterank，用户停留时间、CTR、二跳率等

   **二是 Query-Doc 的特征**：文档对应查询的相关度、每个域的 tf、idf 值，bool model，vsm，bm25，language model 相关度等。

2. **标签获取**

   - **人工标注**

     人工标注比较灵活，但是若需要大量的训练数据，人工标注就不太现实  了，人工标注主要有以下几种标注类型：

     - **单点标注**

       对于每个查询文档直接==打上绝对标签==，即相关度得分；

       二元标注，==相关和不相关==；

       五级标注，按照相关度划分五级（同 NDCG 指标）：即 “最相关”、“相关”、“中性”、“不相关”、最不相关”，通常在模型训练时会用数字来表示，如 1~5

     - **两两标注**

       对于一个查询 Query，标注文档 d1 比文档 d2 是否更加相关，即 (𝑞,𝑑1)≻(𝑞,𝑑2)?

     - **列表标注**

       对于一个查询 Query，将人工理想的排序全部标好

     - **问题**

       人工复杂

   - **日志抽取**

     当搜索引擎搭建起来之后，就可以通过用户点击记录来获取训练数据。对应查询返回的搜索结果，用户会点击其中的某些网页，我们可以==假设用户优先点击的是和查询关键词更相关的网页==

     - **问题**

       用户总是习惯于从上到下浏览搜索结果（产生了bias）

       用户点击有比较大的噪声

       一般头查询（head query）才存在用户点击


**五 排序学习设计方法**

> 排序学习的模型通常分为单点法（Pointwise Approach）、配对法（Pairwise Approach）和列表法（Listwise Approach）三大类，三种方法并不是特定的算法，而是==排序学习模型的设计思路==，主要区别体现在==损失函数==（Loss Function）、以及相应的==标签标注方式==和==优化方法==的不同

1. **单点法（Pointwise）**

   单点法排序学习模型的每一个训练样本都仅仅是某一个查询关键字和某一个文档的配对。单点法将文档转换为特征向量后，机器学习系统根据从训练数据中学习到的分类或者回归函数对文档打分，打分结果即是搜索结果。

   - **分类损失函数**

     输出空间包含的是无序类别，对每个查询-文档对的样本判断是否相关，可以是二分类的，如==相关认为是正例==，不相关认为是负例；

   - **回归损失函数**

     输出空间包含的是真实值相关度得分，可==通过回归来直接拟合相关度==打分。

   - **有序分类损失函数**

     有序分类（Ordinal Classification）：有序分类也称有序回归（Ordinal Regression），输出空间一般包含的是有序类别，<u>通常的做法是找到一个打分函数，然后用一系列阈值对得分进行分割，得到有序类别</u>

   - **Pointwise的优势**

     回到我们的推荐系统领域，==最常用就是二元分类的 Pointwise==，比如常见的点击率（CTR）预估问题，之所以用得多，是因为二元分类的 Pointwise 模型的==复杂度==通常比 Pairwise 和 Listwise 要==低==，而且可以==借助用户的点击反馈自然地完成正负样例的标注==，而其他 Pairwise 和 Listwise 的模型标注就没那么容易了。==成功地将排序问题转化成分类问题==，也就意味着我们机器学习中那些常用的分类方法都可以直接用来解决排序问题，如 LR、GBDT、SVM 等，甚至包括结合深度学习的很多推荐排序模型，都属于这种 Pointwise 的思想范畴。

2. **配对法（Pairwise）**

   配对法的基本思路是==对样本进行两两比较，构建偏序文档对，从比较中学习排序==，因为对于一个查询关键字来说，最重要的其实不是针对某一个文档的相关性是否估计得准确，而是要能够正确估计一组文档之间的 “相对关系”。

   每一个数据样本其实是一个比较关系，当前一个文档比后一个文档相关排序更靠前的话，就是正例，否则便是负例，如下图。试想，有三个文档：A、B 和 C。完美的排序是 “B>C>A”。我们希望通过学习两两关系 “B>C”、“B>A” 和 “C>A” 来重构 “B>C>A”。
   
   - **非常关键的强假设**
   
     - **我们可以针对某一个关键字得到一个完美的排序关系**
   
       对于查询关键字 “苹果”，有的用户的意图是购买水果，有的用户则是打算购买苹果手机，甚至对于同一个用户，在不同时期，对于同一个关键词的搜索意图也可能是不一样的，显然，这类情况就不存在一个完美排序。
   
     - **我们寄希望能够学习文档之间的两两配对关系从而 “重构” 这个完美排序**
   
       在预测的时候，即使模型能够正确判断 “B>C” 和 “C>A”，也不代表模型就一定能得到 “B>A”。
   
     - **我们能够构建样本来描述这样的两两相对的比较关系**
   
       一个相对比较简单的情况，==认为文档之间的两两关系来自于文档特征（Feature）之间的差异==。也就是说，可以利用样本之间特征的差值当做新的特征，从而学习到差值到相关性差异这样的一组对应关系。
   
     - **不同类型的人工标注标签如何转换到 Pairwise 类方法的输出空间**
   
       Pairwise 方法的输出空间应该是包括所有文档的两两文档对的偏序关系（pairwise preference），其取值为 {+1,−1}，+1 表示文档对中前者更相关，-1 则表示文档对中后者更相关
   
       - 对于单点标注，比如相关度打分 𝑙𝑗，文档对 (𝑥𝑢,𝑥𝑣) 的真实标签可定义为 𝑦𝑢,𝑣=2∗𝐼{𝑙𝑢≻𝑙𝑣}−1；
       - 对于两两标注，本身已经是文档对的偏序标签 𝑙𝑢,𝑣，可直接作为真实标签，文档对 (𝑥𝑢,𝑥𝑣) 的真实标签可定义为 𝑦𝑢,𝑣=𝑙𝑢,𝑣；
       - 对于列表标注，拿到的是整体排序 𝜋𝑙，文档对 (𝑥𝑢,𝑥𝑣) 的真实标签可定义为 𝑦𝑢,𝑣=2∗𝐼{π𝑙(𝑢)<π𝑙(𝑣)}−1。
